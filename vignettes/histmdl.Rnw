\documentclass[article,nojss]{jss}

\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{algorithm,algorithmic}
\usepackage{subcaption}
\usepackage{thumbpdf}
%% need no \usepackage{Sweave.sty}
%% \VignetteIndexEntry{histmdl}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem*{assumption}{Assumption}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter{\parens}{\lparen}{\rparen}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bigO}{\mathcal{O}}
%\newcommand{\length}[1]{{\lvert#1\rvert}}
\newcommand{\IN}{\textbf{in} }
\newcommand{\proc}{\textsc}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\rmdata}{\mathrm{data}}
\newcommand{\rmhist}{\mathrm{hist}}
\newcommand{\rmin}{\mathrm{in}}
\newcommand{\rmout}{\mathrm{out}}
\newcommand{\var}[1]{\ensuremath{\mathit{#1}}}


\author{Jouke Witteveen\\Universiteit van Amsterdam}
\title{\pkg{histmdl}: An \proglang{R} Package for Histogram-Like Data~Visualization}
\Plainauthor{Jouke Witteveen} %% comma-separated
\Plaintitle{histmdl: An R Package for Histogram-Like Data Visualization}
\Shorttitle{\pkg{histmdl}: Histogram-Like Data Visualization}

\Abstract{
  We present a histogram-inspired visualization scheme for numeric data.
  The resulting visualizations are shown to have several advantages over ordinary histograms.
  The scheme is based on the minimum description length principle from information theory and hinges on a novel generalization of the Kraft--McMillan theorem.
  Effectively, the scheme performs binning by compression.
  An implementation is made available in the form of the \proglang{R} package \pkg{histmdl}.
}
\Keywords{data visualization, density estimation, histograms, minimum description length}

\Address{
  Jouke Witteveen\\
  Institute for Logic, Language and Computation\\
  Universiteit van Amsterdam\\
  1090 GE, Amsterdam, the Netherlands\\
  E-mail: \email{j.e.witteveen@uva.nl}\\
  URL: \url{https://staff.fnwi.uva.nl/j.e.witteveen/}
}

\begin{document}
<<echo=false>>=
options (prompt="R> ", continue="+ ", width=70, useFancyQuotes=FALSE)
library (histmdl)
set.seed (28011988)
@
\section{Introduction}
\label{sec:introduction}
The classical histogram is a useful tool for getting an impression of the distribution of data.
Its graphical nature gives it a considerable advantage over textual representation in the form of tables.
By its simplicity, the classical histogram is computationally easy to work with and more sensitive to non-smooth structural properties such as outliers than smoothing methods such as those involving kernels.

Once normalized, a histogram can be thought of as a piecewise constant probability density function.
Thus, generating a histogram comes down to finding a piecewise constant probability density function that is a reasonable estimate of a probability density function assumed to have generated the data at hand.
Usually, this estimation is done by settling on a set of discontinuity points, which, in the context of histograms, are called \emph{breakpoints}.
The density between breakpoints is then chosen in accordance with the maximum likelihood estimate, that is, as the fraction of data that falls between the breakpoints.
In this sense, generating a histogram consists of a discretization step and the definition of a multinomial distribution on the resulting bins.

Commonly, breakpoints are restricted to be equidistant and all effort is put in determining the distance between consecutive breakpoints.
Around 1980, methods were proposed, and proven optimal under certain assumptions, to find a suitable bin width for given data \citep{scott1979,freedman1981}.
However, histograms with constant bin width are known to be overly smooth at high density regions and they do not respond well to spikes \citep{denby2009}.
Moreover, histograms with constant bin width can be suggestive of features that are insignificant.
These effects are demonstrated in Figure~\ref{fig:shortcomings}, which shows regular histograms of data sampled as follows, using \proglang{R} \citep{R2014}.
<<>>=
x1 <- c (rnorm (1000, -6), rnorm (1000, 6))
x2 <- c (runif (50), runif (50, max=3))
@
For the bimodal distribution underlying \code{x1}, the binning of the histogram is too coarse.
For the mixed uniform distribution underlying \code{x2}, the histogram gives an overly complex image.
\begin{figure}[htb]
  \begin{center}
    \begin{subfigure}{0.48\textwidth}
<<echo=false,fig=true,height=4,width=3>>=
par (mar=c (4, 4, 0, 0) + 0.2)
x1x <- seq (-10, 10, length=2000)
hist (x1, freq=FALSE, ylim=c (0, .225), main='')
lines (x1x, (dnorm (x1x, -6) + dnorm (x1x, 6)) / 2, lty=2)
@
    \end{subfigure}
    ~
    \begin{subfigure}{0.48\textwidth}
<<echo=false,fig=true,height=4,width=3>>=
par (mar=c (4, 4, 2, 0) + 0.2)
hist (x2, freq=FALSE, main='')
lines (c (0, 1, 1, 3), c (2/3, 2/3, 1/6, 1/6), lty=2)
@
    \end{subfigure}
  \end{center}
  \caption{
    Regular histograms of data sampled according to mixed distributions.
    The generating density functions are shown using dashed lines.
  }
  \label{fig:shortcomings}
\end{figure}

As a remedy to the shortcomings just identified, we propose a method of determining breakpoints and densities that is based on local features of the data, instead of on global features such as the number of data points, the standard deviation \citep{scott1979}, or the inter-quartile range \citep{freedman1981}.
Rather than looking at breakpoints in isolation, we will take the density between potential breakpoints into consideration too.
Our piecewise constant density function will come about as a series of nested closed intervals to which densities are associated.
Thus, our type of histogram is generated by the iterative selection of closed intervals, until some stopping criterion is met.
The selection and stopping criteria are based on the compression of the data attainable using the constructed density function.
While the application of complexity-based reasoning to density estimation is not new \citep{hall1988}, we apply a novel generalization of the Kraft--McMillan theorem that allows us to accurately penalize the addition of extra breakpoints.
In doing so, our approach is an application of the minimum description length principle (\emph{MDL}) \citep{grunwald2007}.


\section{Encoding with densities}
In probability theory, MDL-based reasoning depends on the Kraft--McMillan theorem \citep{Cover1991}, which relates probabilities to code lengths.
Although the usual formulation of the Kraft--McMillan theorem is about probability mass functions, the theorem can be generalized to cover probability density functions \citep{witteveen2014}.
In effect, this allows MDL-based reasoning in settings with continuous random variables.
We include the generalization of the Kraft--McMillan theorem here, together with an informal proof.
\begin{theorem}
  A function $\ell: \bbR \to \bbR$ that behaves like a code length is induced by
 a density function $p$ on $\bbR$ as follows:
  \begin{equation*}
    \ell(x) = -\log p(x) + k,
  \end{equation*}
  where $k$ is a discretization constant that goes to infinity as the fineness of the discretization increases.
\label{thm:realKraft}
\end{theorem}
\begin{proof}
  By the \emph{Radon--Nikodym~theorem}, we can think of the probability of $x \in \bbR$ under $p$ as $p(x)\rmd\lambda(x)$, where $\lambda$ denotes the Lebesgue measure on $\bbR$.
  The Lebesgue measure is translation invariant, thus we can drop the dependency on $x$.
  This yields $p(x)\rmd\lambda$ as an expression for the probability of $x$ under $p$.
  The \emph{Kraft--McMillan~theorem} now prescribes a function that behaves like a code length, of the form $\ell(x) = -\log(p(x)\rmd\lambda) = -\log p(x) - \log \rmd\lambda$.
  It is common to think of $\rmd\lambda$ as a limit of a sequence monotonically approaching zero from above.
  Using this convention, $-\log\rmd\lambda$ is the limit of a sequence monotonically approaching infinity.
  As the sequence does not depend on $x$, the term does not influence the behavior of $\ell(x)$, which justifies the expression in the theorem.
\end{proof}

Given some data and a probability density function, Theorem~\ref{thm:realKraft} enables us to determine, up to a constant, the code length needed to describe the data with respect to the probability density function.
Denoting the data by $\omega$ and the probability density function by $H$, we will use $\ell_\rmdata(\omega \mid H)$ for this code length.
Note that $\ell_\rmdata(\omega \mid H)$ is thus only defined up to an additive constant.
If we restrict to piecewise constant $H$, we could say that $\ell_\rmdata(\omega \mid H)$ captures the complexity of data $\omega$ under histogram-like probability density function $H$.
In order to apply MDL for the selection of a histogram-like probability density function, we confine our attention to a class of piecewise constant probability density functions for which we will define a complexity measure $\ell_\rmhist$.
Selection then proceeds by trying to find a function $H$ in this class that minimizes the two-part code length
\begin{equation}
  \label{eq:twopart}
  \ell_\rmhist(H) + \ell_\rmdata(\omega \mid H).
\end{equation}

As said, our class will consist of probability density functions defined by nested closed intervals, where each interval is provided with a density.
For mathematical convenience, we will not consider the specification of densities in our complexity measure $\ell_\rmhist$.
In Section~\ref{sec:analysis} we will see that our implementation allows for the reservation of a code length for the specification of densities up to some precision.
Thus, we define $\ell_\rmhist$ recursively as the code length of a specification of nested intervals.
Given a closed interval $I \subset \bbR$, we use a single bit to encode whether or not there is an additional closed interval, $I' \subseteq I$.
If there is, we encode it and recurse on it.
Moreover, we also recurse on the outside of $I'$.
That is, we transform $I$ by contracting $I'$ to a point and recurse on this transformed interval.
For the specification of $I'$ inside $I$, we use a uniform distribution on the space of possible interval endpoints inside $I$.
The corresponding probability density function takes on the value $\frac{2}{\lambda(I)^2}$ on intervals inside $I$, hence, by Theorem~\ref{thm:realKraft}, the code length of specifying $I'$ this way is
\begin{equation*}
  -\log\frac{2}{\lambda(I)^2} + k_\rmhist.
\end{equation*}
Thus, if the root of the nested intervals of some $H$ is $I$ and we designate, if applicable, the two constituents of the recursion by $H_\rmin$ and $H_\rmout$, we get, in bits,
\begin{equation*}
  \ell_\rmhist(H) =\begin{cases}
    1 - \log\frac{2}{\lambda(I)^2} + k_\rmhist + \ell_\rmhist(H_\rmin) + \ell_\rmhist(H_\rmout)    & \text{with further nesting} \\
    1	& \text{otherwise}
  \end{cases}
\end{equation*}

Note that the above defines a proper code length to the class of nested intervals, instead of to the class of probability density functions based on nested intervals.
However, we feel that this code adequately balances representing our interests and being mathematically pleasant to work with.


\section{Estimation}
For data $\omega$, we are after histogram-like probability density functions $H$ that minimize \eqref{eq:twopart}.
We can approximate such $H$ greedily, one interval at a time.
Therefore, we want to find an $\hat{H}$ with only one level of nesting that minimizes $\ell_\rmhist(\hat{H}) + \ell_\rmdata(\omega \mid \hat{H})$.
Since $\ell_\rmhist(\hat{H})$ takes on the same value for all $\hat{H}$ that have only one level of nesting, only $\ell_\rmdata(\omega \mid \hat{H})$ is relevant for our minimization.
Now, let $I$ be the root interval of $\hat{H}$ and assume $I$ covers all of $\omega$.
Further, let $I'$ be the interval inside $I$ in $\hat{H}$, let $n$ be the size of $\omega$, and $n_\rmin$ the number of elements of $\omega$ that fall within $I'$.
Setting $n_\rmout = n - n_\rmin$ and denoting by $p_\rmin$ and $p_\rmout$ the probability densities inside and outside $I'$ respectively, we obtain, by Theorem~\ref{thm:realKraft},
\begin{equation*}
  \ell_\rmdata(\omega \mid \hat{H}) = - n_\rmin\log p_\rmin - n_\rmout\log p_\rmout + n k_\rmdata.
\end{equation*}
Note that probability densities are maximized, and the code length thus minimized, by taking $I$ as small as possible and taking maximum likelihood estimates for $p_\rmin$ and $p_\rmout$.
Therefore, we take the extreme values of $\omega$ as endpoints of $I$ and set $p_\rmin$ and $p_\rmout$ to $\frac{n_\rmin}{\lambda(I')}$ and $\frac{n_\rmout}{\lambda(I) - \lambda(I')}$ respectively.
The main variable in our minimization thus becomes $I'$.
Finding an optimal $I'$ given $\omega$ is simplified by the following.
\begin{assumption}
  We assume that the optimal $I'$ is so that we have $p_\rmin > p_\rmout$.
  This holds for instance when a sufficiently smooth distribution with concave features underlies $\omega$.
\end{assumption}
With this assumption in place, the optimal $I'$ has elements of $\omega$ as its endpoints.
Indeed, intervals of this kind maximize the inside data density.
Under the above assumption, finding the interval with the lowest corresponding code length is possible using an algorithm based on Kruskal's Minimum Spanning Tree algorithm \citep{cormen1991}.
Our algorithm, Algorithm~\ref{alg:bestinterval}, uses a disjoint-set data structure to keep track of locally optimal intervals and merges them to get to a globally optimal interval.
As it is based on Kruskal's algorithm, our algorithm has a running time in $\bigO(n\log n)$, which we consider attractive.
\begin{algorithm}
  \caption{\proc{Best-Interval}($\omega$)}
  \label{alg:bestinterval}
  \begin{algorithmic}
  \FOR{each $x$ \IN $\omega$}
    \STATE \proc{Make-Set}($x$)
    \STATE $\var{best}[x] \gets$ the interval covering only $x$
  \ENDFOR
  \STATE\COMMENT{we assume $\omega$ is sorted}
  \FOR{each neighbouring $x, y$ \IN $\omega$, in increasing order of distance between them,}
    \STATE $I_x \gets \var{best}[\proc{Find-Set}(x)]$
    \STATE $I_y \gets \var{best}[\proc{Find-Set}(y)]$
    \STATE $I_u \gets$ the smallest interval covering $I_x$ and $I_y$
    \STATE $u \gets \proc{Union}(x, y)$
    \STATE\COMMENT{we abuse notation slightly by using $I'$ in place of $\hat{H}$}
    \STATE $\var{best}[u] \gets \argmin_{I' \in \{I_x, I_y, I_u\}} \ell_\rmdata(\omega \mid I')$
  \ENDFOR
  \RETURN $\var{best}[u]$
  \end{algorithmic}
\end{algorithm}

We want to apply Algorithm~\ref{alg:bestinterval} recursively and for that we need to know when to stop our recursion.
Given an $\hat{H}$ that has a single level of nesting, we compare it to $\mathring{H}$ which has no nesting, and prefer the $H \in \{\hat{H}, \mathring{H}\}$ that roughly minimizes $\ell_\rmhist(H) + \ell_\rmdata(\omega \mid H)$.
We say roughly, since we will discard the cost of recursion in $\ell_\rmhist$ because our candidate $\hat{H}$ was restricted to a single level of nesting a priori.
With $I$, $I'$, $n$, $n_\rmin$ and $n_\rmout$ as before, we have
\begin{align*}
  \ell_\rmhist(\hat{H}) + \ell_\rmdata(\omega \mid \hat{H}) &= 1 - \log\frac{2}{\lambda(I)^2} + k_\rmhist \\&\phantom{=\ }- n_\rmin\log\frac{n_\rmin}{\lambda(I')} - n_\rmout\log\frac{n_\rmout}{\lambda(I) - \lambda(I')} + n k_\rmdata, \\
  \ell_\rmhist(\mathring{H}) + \ell_\rmdata(\omega \mid \mathring{H}) &= 1 - n\log\frac{n}{\lambda(I)} + n k_\rmdata.
\end{align*}
Note how a comparison of $\hat{H}$ and $\mathring{H}$ is possible without deciding on $k_\rmdata$, as it influences both code lengths in the same way.
We set $k_\rmhist$ to $-2\log\frac{\lambda(I)}{n}$, for which \citet{witteveen2014} give a rationale.
Consequently, the difference between the above two code lengths becomes
\begin{equation}
  \label{eq:difference}
  -\log\frac{2}{n^2} - n_\rmin\log\frac{n_\rmin}{\lambda(I')} - n_\rmout\log\frac{n_\rmout}{\lambda(I) - \lambda(I')} + n\log\frac{n}{\lambda(I)}.
\end{equation}
If, for some $I'$, this difference turns out negative, $I'$ is deemed informative and we will proceed with our recursion.
The resulting procedure is included as Algorithm~\ref{alg:recurse}, which computes a histogram-like probability density function fitting data $\omega$.
\begin{algorithm}
  \caption{\proc{Recursive-Intervals}($\omega$)}
  \label{alg:recurse}
  \begin{algorithmic}
  \STATE Initialize $H$ with the smallest interval covering $\omega$
  \STATE $I' \gets \proc{Best-Interval}(\omega)$
  \IF{$I'$ makes \eqref{eq:difference} negative}
    \STATE recurse on $\omega \cap I'$
    \STATE contract $\omega \cap I'$ to a single point and recurse
    \STATE undo the contraction and add the recursion outcomes to $H$
  \ELSE
    \STATE ignore $I'$, as it is not informative
  \ENDIF
  \RETURN $H$
  \end{algorithmic}
\end{algorithm}


\section{Analysis}
\label{sec:analysis}
The method outlined in Algorithm~\ref{alg:recurse} follows the divide and conquer paradigm.
Precisely how often the algorithm recurses depends on the whimsicality of the data.
The more there is to show about the data, the longer it takes to generate our nested intervals.
Nevertheless, the running time of Algorithm~\ref{alg:bestinterval} ensures that our method has a worst case running time that is polynomial in the number of data points.

The procedure of estimating histogram-like probability density functions as described in the previous section is implemented in the \proglang{R} package \pkg{histmdl}.
Using two parameters, it is possible to tune the behavior of the \code{histmdl} function.
These parameters mainly serve to filter spurious spikes.
The first parameter is \code{support=}, which is responsible for lower bounding the number of data points in every interval.
The second parameter is \code{gain=}, which is added to \eqref{eq:difference} and thus places a lower bound on the compression obtained per additional interval.
This parameter enables us to account for the code length needed to specify the densities inside intervals, which we ignored in our framework.

Even though our examples in Section~\ref{sec:introduction} were somewhat contrived, it is nice to see that our new visualization successfully addresses the shortcomings we have identified.
<<eval=false>>=
library (histmdl)
histmdl (x1, gain=2)
histmdl (x2)
@
\begin{figure}[htb]
  \begin{center}
    \begin{subfigure}{0.48\textwidth}
<<echo=false,fig=true,height=4,width=3>>=
par (mar=c (4, 4, 0, 0) + 0.2)
histmdl (x1, gain=2, xlim=c(-10, 10), ylim=c (0, .225), main='', col='lightblue')
hist (x1, freq=FALSE, add=TRUE)
lines (x1x, (dnorm (x1x, -6) + dnorm (x1x, 6)) / 2, lty=2)
@
    \end{subfigure}
    ~
    \begin{subfigure}{0.48\textwidth}
<<echo=false,fig=true,height=4,width=3>>=
par (mar=c (4, 4, 2, 0) + 0.2)
histmdl (x2, xlim=c (0, 3), main='', col='lightblue')
hist (x2, freq=FALSE, add=TRUE)
lines (c (0, 1, 1, 3), c (2/3, 2/3, 1/6, 1/6), lty=2)
@
    \end{subfigure}
  \end{center}
  \caption{
    Our new histogram-like visualizations added to Figure~\ref{fig:shortcomings}.
  }
  \label{fig:improvement}
\end{figure}
In generating the Figure~\ref{fig:improvement}, we have increased the \code{gain=} parameter a little for data \code{x1} to filter two spikes covering $4$ and $8$ elements respectively.
Using only $9$ bins instead of $11$, as used by the classical histogram, our new visualization for \code{x1} follows the generating density function more closely than the classical histogram.
The new visualization manages to increase detail precisely where relevant.
For \code{x2}, the new visualization closely reconstructs the density function that was used to generate the data.
This is not entirely surprising, as the class of piecewise constant probability density functions which our method deals with consists of probability density functions just like the one we used to generate \code{x2}.
Yet the new visualization slightly overestimates the densities.
This is to be expected by the greedy nature of Algorithm~\ref{alg:bestinterval}, which was motivated by an urge to maximize the likelihood of the data.
Consequently, the new visualization for \code{x2} is based on an interval $[0.019, 2.934]$ with a subinterval $[0.019, 0.981]$.
Indeed, these intervals are a little narrower than $[0, 3]$ and $[0,1]$, respectively.

For a less contrived demonstration, we turn to a small real-world dataset: eruption data of the Old Faithful geyser.
It is known that the dataset is a little distorted.
Rounding and measurement errors are both present.
To overcome the rounding, we reconstruct the eruption duration in seconds.
Although the data is thus discrete, there are $124$ different values, most occurring only once, which is enough to favor our approach for continuous data.
The resulting Figure~\ref{fig:faithful} does not give an indication to worry much about measurement errors.
<<eval=false>>=
histmdl (round (60 * faithful$eruptions))
hist (round (60 * faithful$eruptions))
@
\begin{figure}[htb]
  \begin{center}
<<echo=false,fig=true>>=
histmdl (round (60 * faithful$eruptions), xlim=c (80, 320), main='', col='lightblue')
hist (round (60 * faithful$eruptions), freq=FALSE, add=TRUE)
@
  \end{center}
  \caption{An ordinary histogram and our new histogram-like visualization of the eruption durations in seconds of the Old Faithful geyser.}
  \label{fig:faithful}
\end{figure}
What catches the eye in Figure~\ref{fig:faithful} is the spike between $107$ and $113$ seconds.
This spike covers $30$ eruptions and is hardly present in the ordinary histogram.
Contrary to seeing similar spikes in ordinary histograms, such spikes in histogram-like visualizations of our new kind deserve attention.
After all, they survived a filter on their contribution to compressibility.
In other words, they were deemed significant by the method.
We will not research this particular spike further, but do remark that such observations add to the value of our histogram-like visualizations.


\section{Summary}
We identified two shortcomings in ordinary histograms, namely that they easily become too coarse for data sampled from multimodal distributions, and that they can present an overly complex image depicting insignificant density fluctuations.
As a remedy, we introduced a class of histogram-like probability density functions and an associated method for the estimation of such functions given some data.
This method is implemented in the \proglang{R} package \pkg{histmdl} and is shown able to overcome the shortcomings identified.


\section*{Acknowledgements}
We thank Richard Gill for preaching the \proglang{R} language.

\bibliography{histmdl}
\end{document}
